<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Sentiment Analysis of A Million News Headlines | A blog on Data</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/about/">About</a></li>
      
      <li><a href="/categories/">Categories</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">Sentiment Analysis of A Million News Headlines</span></h1>

<h2 class="date">2017/11/26</h2>
</div>

<main>


<p>I&rsquo;ve just finished <a href="http://r4ds.had.co.nz/">R for Data Science</a> by Hadley Wickham and just started <a href="http://tidytextmining.com/">Text mining With R</a> by Julia Silge. So I figured it&rsquo;s about time i do some data analysis to apply the skills I learned. I decided to do sentiment analysis after reading this <a href="https://juliasilge.com/blog/life-changing-magic/">post</a> by Julia Silge.</p>

<p>After skimming through some interesting datasets on the internet, i decided to use &lsquo;A Million Headlines` dataset which can be found on Kaggle. It&rsquo;s a dataset of news headlines published over a period of 14 years from 2003 to 2017 taken from Australian news source ABC(Australian Broadcasting Group).</p>

<p>First, let&rsquo;s import all the packages needed:</p>

<pre><code class="language-r">library(tidyverse)
library(here)
library(tidytext)
library(viridis)
library(widyr)
library(ggraph)
library(igraph)
library(scales)
library(knitr)
library(wordcloud)
library(reshape2)
</code></pre>

<p>Now let&rsquo;s import the data</p>

<pre><code class="language-r"># import data
news &lt;- as.tibble(read_csv(here(&quot;abcnews-date-text.csv&quot;)))
news
</code></pre>

<pre><code>## # A tibble: 1,093,281 x 2
##    publish_date headline_text                                     
##           &lt;int&gt; &lt;chr&gt;                                             
##  1     20030219 aba decides against community broadcasting licence
##  2     20030219 act fire witnesses must be aware of defamation    
##  3     20030219 a g calls for infrastructure protection summit    
##  4     20030219 air nz staff in aust strike for pay rise          
##  5     20030219 air nz strike to affect australian travellers     
##  6     20030219 ambitious olsson wins triple jump                 
##  7     20030219 antic delighted with record breaking barca        
##  8     20030219 aussie qualifier stosur wastes four memphis match 
##  9     20030219 aust addresses un security council over iraq      
## 10     20030219 australia is locked into war timetable opp        
## # ... with 1,093,271 more rows
</code></pre>

<h1 id="term-frequency">Term Frequency</h1>

<p>One of the common task in text mining is to look at word frequencies. Let&rsquo;s analyze word frequencies in all of the headlines</p>

<pre><code class="language-r">news &lt;- news %&gt;%
      # create year column    
      mutate(year = substr(publish_date, 
                start = 1, stop = 4),
                linenumber = row_number())

news
</code></pre>

<pre><code>## # A tibble: 1,093,281 x 4
##    publish_date headline_text                              year  linenumb…
##           &lt;int&gt; &lt;chr&gt;                                      &lt;chr&gt;     &lt;int&gt;
##  1     20030219 aba decides against community broadcastin… 2003          1
##  2     20030219 act fire witnesses must be aware of defam… 2003          2
##  3     20030219 a g calls for infrastructure protection s… 2003          3
##  4     20030219 air nz staff in aust strike for pay rise   2003          4
##  5     20030219 air nz strike to affect australian travel… 2003          5
##  6     20030219 ambitious olsson wins triple jump          2003          6
##  7     20030219 antic delighted with record breaking barca 2003          7
##  8     20030219 aussie qualifier stosur wastes four memph… 2003          8
##  9     20030219 aust addresses un security council over i… 2003          9
## 10     20030219 australia is locked into war timetable opp 2003         10
## # ... with 1,093,271 more rows
</code></pre>

<p>we can use <code>unnest_tokens</code> to separate each line into words. The default tokenizing is for words, but other options include characters, sentences, lines, paragraphs, or separation around regex pattern.</p>

<pre><code class="language-r">tidy_news &lt;- news %&gt;% 
        unnest_tokens(word, headline_text)

tidy_news
</code></pre>

<pre><code>## # A tibble: 6,988,120 x 4
##    publish_date year  linenumber word        
##           &lt;int&gt; &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;       
##  1     20030219 2003           1 aba         
##  2     20030219 2003           1 decides     
##  3     20030219 2003           1 against     
##  4     20030219 2003           1 community   
##  5     20030219 2003           1 broadcasting
##  6     20030219 2003           1 licence     
##  7     20030219 2003           2 act         
##  8     20030219 2003           2 fire        
##  9     20030219 2003           2 witnesses   
## 10     20030219 2003           2 must        
## # ... with 6,988,110 more rows
</code></pre>

<p>Now we can manipulate the data and do term frequency analysis. First, let&rsquo;s remove stop words which can be obtain from dataset <code>stop_words</code> with the function <code>anti_join</code>. Stop words are words which do not contain important significance. We filter out stop words as it could affect our analysis.</p>

<pre><code class="language-r"># remove stopwords

data(&quot;stop_words&quot;)
tidy_news &lt;- tidy_news %&gt;%
        anti_join(stop_words)
</code></pre>

<p>Let&rsquo;s see the most frequent words use in the news headlines since 2003:</p>

<pre><code class="language-r"># most common words
tidy_news %&gt;%
    count(word, sort = TRUE) %&gt;%
    head(20) %&gt;%
    mutate(word = reorder(word, n)) %&gt;%
    ggplot(aes(word, n)) +
    geom_bar(stat = &quot;identity&quot;) +
    coord_flip() +
    ylab(&quot;Number of occurences&quot;) +
    xlab(&quot;Word&quot;)
</code></pre>

<p><img src="2017-11-26-sentiment-analysis-on-a-million-news-headlines_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>

<p>We can see here most of the headlines contain the words, &ldquo;police&rdquo;, &ldquo;court&rdquo;, &ldquo;council&rdquo;.</p>

<h2 id="network-of-words">Network of Words</h2>

<p>Let&rsquo;s count the words that occur together in the headlines from 2017. Using <code>pair_count</code> function from <code>widyr</code> package, we can count highest co-occurances pair of words.</p>

<pre><code class="language-r">headlines_2017 &lt;- tidy_news %&gt;% 
    filter(year == &quot;2017&quot;) %&gt;%
    pairwise_count(word, linenumber, sort = TRUE)  
  
headlines_2017
</code></pre>

<pre><code>## # A tibble: 792,786 x 3
##    item1    item2        n
##    &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;
##  1 trump    donald     476
##  2 donald   trump      476
##  3 korea    north      238
##  4 north    korea      238
##  5 marriage sex        156
##  6 sex      marriage   156
##  7 election wa         147
##  8 wa       election   147
##  9 turnbull malcolm    138
## 10 malcolm  turnbull   138
## # ... with 792,776 more rows
</code></pre>

<p>Donald trump is the highest occurences pair of words in 2017 followed by North Korea which unsurprising as the feud between them bring fear about nuclear war around the world. Also in 2017, Australia vote in favour of legalising same sex marriage which is big news across the country. Hence explains why sex marriage is just below Donald Trump and North Korea in frequency of co-occurences pair of words in 2017 headlines.</p>

<p>Let&rsquo;s plot the network of words occurences:</p>

<pre><code class="language-r">#pairwise count
word_pairs &lt;- tidy_news %&gt;%
  group_by(word) %&gt;%
  filter(n() &gt; 5) %&gt;%
  ungroup() %&gt;%
  pairwise_count(item=word, 
                 linenumber, sort = TRUE, 
                 upper = FALSE) %&gt;%
  filter(n &gt; 10)

#create plot
word_pairs %&gt;%
  top_n(100) %&gt;%
        graph_from_data_frame() %&gt;%
        ggraph(layout = &quot;fr&quot;) +
        geom_edge_link(aes(edge_alpha = n, 
                           edge_width = n)) +
        geom_node_point(color = &quot;darkslategray4&quot;, 
                        size = 5) +
        geom_node_text(aes(label = name), 
                       vjust = 1.8) +
        ggtitle(expression(paste(
                  &quot;Word Network in ABC Headlines From 
                  2003-2017&quot;))) +
        theme_void()
</code></pre>

<p><img src="2017-11-26-sentiment-analysis-on-a-million-news-headlines_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>

<p>Next, we&rsquo;ll look into sentiment analysis of these words so we can understand what type of sentiment have been used in most of these headlines.</p>

<h2 id="sentiment-analysis">Sentiment Analysis</h2>

<p>Now let&rsquo;s investigate sentiment analysis. When we reads a text, we use our understanding of the emotional intent of words to infer wheter a section of words is positive or negative and also categorized it into emotion like anger or joy. Let&rsquo;s use bing lexicon from <code>sentiments</code> dataset to categorized our words into positive or negative sentiment.</p>

<pre><code class="language-r"># create dataframe of words from bing lexicon
library(tidyr)
bing &lt;- sentiments %&gt;%
        filter(lexicon == &quot;bing&quot;) %&gt;%
        select(-score)

bing
</code></pre>

<pre><code>## # A tibble: 6,788 x 3
##    word        sentiment lexicon
##    &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;  
##  1 2-faced     negative  bing   
##  2 2-faces     negative  bing   
##  3 a+          positive  bing   
##  4 abnormal    negative  bing   
##  5 abolish     negative  bing   
##  6 abominable  negative  bing   
##  7 abominably  negative  bing   
##  8 abominate   negative  bing   
##  9 abomination negative  bing   
## 10 abort       negative  bing   
## # ... with 6,778 more rows
</code></pre>

<p>Using <code>inner_join</code> function, we can categorized the words into positive or negative by joining <code>bing</code> dataset.</p>

<pre><code class="language-r"># classified words into positive 
## or negative based on bing lexicon
news_sentiment &lt;- tidy_news %&gt;%
        inner_join(bing) %&gt;% 
        count(year,sentiment) %&gt;% 
        spread(sentiment, n, fill = 0) %&gt;% 
        mutate(sentiment = positive - negative)

news_sentiment
</code></pre>

<pre><code>## # A tibble: 15 x 4
##    year  negative positive sentiment
##    &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
##  1 2003     26303    12604    -13699
##  2 2004     31093    14291    -16802
##  3 2005     31705    13522    -18183
##  4 2006     28471    12123    -16348
##  5 2007     32875    13454    -19421
##  6 2008     34001    14123    -19878
##  7 2009     32679    13069    -19610
##  8 2010     31273    12589    -18684
##  9 2011     30169    11997    -18172
## 10 2012     30152    13555    -16597
## 11 2013     31885    14523    -17362
## 12 2014     28363    14290    -14073
## 13 2015     30389    14673    -15716
## 14 2016     24249    11487    -12762
## 15 2017     14733     7249    - 7484
</code></pre>

<h2 id="most-common-positive-and-negative-words">Most common positive and negative words</h2>

<p>Now that we have data frame of positive and negative sentiments, we can analyze which words is most common in the positive and negative category. We can filter out <code>NA</code> sentiment or neutral sentiment.</p>

<pre><code class="language-r">word_count &lt;- tidy_news %&gt;%
  left_join(get_sentiments(&quot;bing&quot;), by = &quot;word&quot;) %&gt;%
  filter(!is.na(sentiment)) %&gt;%
  count(word, sentiment, sort = TRUE) %&gt;% 
  ungroup()

top_sentiments_bing &lt;- word_count %&gt;%
          filter(word != &quot;wins&quot;) %&gt;%
          group_by(sentiment) %&gt;%
          top_n(5, n) %&gt;%
          mutate(num = ifelse(sentiment == &quot;negative&quot;,
                              -n, n)) %&gt;%
          mutate(word = reorder(word, num)) %&gt;%
          ungroup()

top_sentiments_bing
</code></pre>

<pre><code>## # A tibble: 10 x 4
##    word    sentiment     n    num
##    &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;  &lt;int&gt;
##  1 crash   negative  11103 -11103
##  2 death   negative  11052 -11052
##  3 murder  negative   9105 - 9105
##  4 win     positive   8246   8246
##  5 killed  negative   8058 - 8058
##  6 attack  negative   7082 - 7082
##  7 boost   positive   6979   6979
##  8 gold    positive   6161   6161
##  9 top     positive   5647   5647
## 10 support positive   5365   5365
</code></pre>

<p>Let&rsquo;s see top 5 words from positive and negative sentiment.</p>

<pre><code class="language-r">ggplot(top_sentiments_bing, aes(reorder(word, num), num,
                                fill = sentiment)) +
  geom_bar(stat = 'identity', alpha = 0.75) + 
  scale_fill_manual(guide = F, values = c(&quot;black&quot;, 
                                          &quot;darkgreen&quot;)) +
  scale_y_continuous(breaks = pretty_breaks(7)) + 
  labs(x = '', y = &quot;Number of Occurrences&quot;,
       title = 'News Headlines Sentiments',
       subtitle = 'Most Common Positive and Negative Words') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1,
                                   size = 14, face = &quot;bold&quot;),
        panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_line(size = 1.1))
</code></pre>

<p><img src="2017-11-26-sentiment-analysis-on-a-million-news-headlines_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>

<h2 id="word-cloud-most-common-positive-and-negative-words-in-news-headlines">Word Cloud: Most Common Positive and Negative Words in News Headlines</h2>

<pre><code class="language-r">library(wordcloud)   # to create wordcloud
library(reshape2)    # for acast() function

tidy_news %&gt;%
  inner_join(bing) %&gt;%
  count(word, sentiment, sort = TRUE) %&gt;%
  acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;%
  comparison.cloud(colors = c(&quot;black&quot;, &quot;darkgreen&quot;), 
                   title.size = 1.5)
</code></pre>

<p><img src="2017-11-26-sentiment-analysis-on-a-million-news-headlines_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>

<h2 id="proportions-of-positive-and-negative-words">Proportions of Positive and Negative Words</h2>

<p>Now let&rsquo;s see the proportions of negative and positive words to entire data set. after filtering out words categorized as neutral, we calculate the frequency by first grouping them along sentiment then counting the rows for each of these groups. Finally, we can calculate the percentage by dividing the sum of all the rows in the data set.</p>

<pre><code class="language-r">sentiment_bing &lt;- tidy_news %&gt;% 
        left_join(get_sentiments(&quot;bing&quot;), by = &quot;word&quot;) %&gt;%
        filter(!is.na(sentiment)) %&gt;%
        group_by(year, sentiment) %&gt;%
        summarise(n = n()) %&gt;%
        mutate(percent = n / sum(n)) %&gt;%
        ungroup()

sentiment_bing
</code></pre>

<pre><code>## # A tibble: 30 x 4
##    year  sentiment     n percent
##    &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt;
##  1 2003  negative  26303   0.676
##  2 2003  positive  12604   0.324
##  3 2004  negative  31093   0.685
##  4 2004  positive  14291   0.315
##  5 2005  negative  31705   0.701
##  6 2005  positive  13522   0.299
##  7 2006  negative  28471   0.701
##  8 2006  positive  12123   0.299
##  9 2007  negative  32875   0.710
## 10 2007  positive  13454   0.290
## # ... with 20 more rows
</code></pre>

<pre><code class="language-r">sentiment_bing %&gt;% 
  ggplot(aes(x = year, y = percent, color = sentiment,
             group = sentiment)) + 
  geom_line(size = 1) + 
  geom_point(size = 3) +
  scale_y_continuous(breaks = pretty_breaks(5), 
                     labels = percent_format()) +
  labs(x = &quot;Album&quot;, y = &quot;Emotion Words Count (as %)&quot;) +
  scale_color_manual(values = c(positive = &quot;darkgreen&quot;, 
                                negative = &quot;black&quot;)) +
  ggtitle(&quot;Proportion of Positive and Negative Words&quot;, 
          subtitle = &quot;Bing lexicon&quot;) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1,
                                   size = 11, face = &quot;bold&quot;),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 11, face = &quot;bold&quot;))
</code></pre>

<p><img src="2017-11-26-sentiment-analysis-on-a-million-news-headlines_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>

<p>The proportion of negative sentiment words has been consistently much higher than proportion of positive sentiment words since 2003.</p>

<p>Let&rsquo;s use NRC lexicon for sentimenLet&rsquo;s use NRC lexicon for sentiment analysis. NRC sentiments got list of sentiments way beyond positive and negative, it categorizes words into eight different emotion terms, <code>Anger</code>, <code>Anticipation</code>, <code>Disgust</code>, <code>Fear</code>, <code>Joy</code>, <code>Sadness</code>, <code>Surprise</code>, and <code>Trust</code>.</p>

<pre><code class="language-r">library(RColorBrewer)
cols &lt;- colorRampPalette(brewer.pal(n = 8, name = &quot;Set1&quot;))(8)

cols
</code></pre>

<pre><code>## [1] &quot;#E41A1C&quot; &quot;#377EB8&quot; &quot;#4DAF4A&quot; &quot;#984EA3&quot; &quot;#FF7F00&quot; &quot;#FFFF33&quot; &quot;#A65628&quot;
## [8] &quot;#F781BF&quot;
</code></pre>

<p>Now, let&rsquo;s plot the distribution of emotion terms on boxplot:</p>

<pre><code class="language-r">cols &lt;- c(&quot;anger&quot; = &quot;#E41A1C&quot;, &quot;sadness&quot; = &quot;#377EB8&quot;,
          &quot;disgust&quot; = &quot;#4DAF4A&quot;, &quot;fear&quot; = &quot;#984EA3&quot;, 
          &quot;surprise&quot; = &quot;#FF7F00&quot;, &quot;joy&quot; = &quot;#FFFF33&quot;, 
          &quot;anticipation&quot; = &quot;#A65628&quot;, &quot;trust&quot; = &quot;#F781BF&quot;)

news_nrc &lt;- tidy_news %&gt;% 
  left_join(get_sentiments(&quot;nrc&quot;), by = &quot;word&quot;) %&gt;% 
  filter(!(sentiment == &quot;negative&quot; | 
           sentiment == &quot;positive&quot;)) %&gt;% 
  mutate(sentiment = as.factor(sentiment)) %&gt;% 
  group_by(index = linenumber %/% 100, 
           sentiment) %&gt;% 
  summarize(n = n()) %&gt;% 
  mutate(percent = n / sum(n)) %&gt;%   
  select(-n) %&gt;% 
  ungroup() 

library(hrbrthemes)

news_nrc %&gt;% 
  ggplot() +
  geom_boxplot(aes(x = reorder(sentiment, percent), 
                   y = percent, fill = sentiment)) +
  scale_y_continuous(breaks = pretty_breaks(5), 
                     labels = percent_format()) +
  scale_fill_manual(values = cols) +
  ggtitle(&quot;Distribution of Emotion Terms&quot;, 
          subtitle = &quot;n = 11 (Albums)&quot;) +
  labs(x = &quot;Emotion Term&quot;, y = &quot;Percentage&quot;) +
  theme_bw() +
  theme(legend.position = &quot;none&quot;,
        axis.text.x = element_text(size = 11, 
                                   face = &quot;bold&quot;),
        axis.text.y = element_text(size = 11, 
                                   face = &quot;bold&quot;))
</code></pre>

<p><img src="2017-11-26-sentiment-analysis-on-a-million-news-headlines_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>

<p>Fear has highest percentage in the distribution. Next, we can see how the sentiment emotions of headlines change over time by creating bump chart that plots different sentiment groups</p>

<pre><code class="language-r">news_nrc2 &lt;- tidy_news %&gt;% 
  left_join(get_sentiments(&quot;nrc&quot;), by = &quot;word&quot;) %&gt;% 
  filter(!(sentiment == &quot;negative&quot; | 
             sentiment == &quot;positive&quot;)) %&gt;% 
  mutate(sentiment = as.factor(sentiment)) %&gt;% 
  group_by(year, sentiment) %&gt;% 
  summarize(n = n()) %&gt;% 
  mutate(percent = n / sum(n)) %&gt;%   
  select(-n) %&gt;% 
  ungroup() 


news_nrc2 %&gt;% 
  group_by(year) %&gt;%
  ggplot(aes(year, percent, color = sentiment, 
             group = sentiment)) +
  geom_line(size = 1) +
  geom_point(size = 3.5) +
  scale_y_continuous(breaks = pretty_breaks(5), 
                     labels = percent_format()) +
  xlab(&quot;Year&quot;) + ylab(&quot;Proportion of Emotion Words&quot;) +
  ggtitle(&quot;News Headlines Sentiments Across Years&quot;, 
          subtitle = &quot;From 2000-2016&quot;) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, 
                                   hjust = 1, size = 11, 
                                   face = &quot;bold&quot;),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 11, 
                                   face = &quot;bold&quot;)) +
  scale_color_brewer(palette = &quot;Set1&quot;)
</code></pre>

<p><img src="2017-11-26-sentiment-analysis-on-a-million-news-headlines_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>

<p>We can see that the sentiment changes over time is quite consistent, with fear sentiment already at high level in 2003.</p>

<p>Let&rsquo;s see what are the most words used that are associated with fear:</p>

<pre><code class="language-r">nrc_fear &lt;- get_sentiments(&quot;nrc&quot;) %&gt;% 
  filter(sentiment == &quot;fear&quot;)

tidy_news %&gt;% 
  inner_join(nrc_fear) %&gt;% 
  count(word, sort = TRUE)
</code></pre>

<pre><code>## # A tibble: 1,297 x 2
##    word           n
##    &lt;chr&gt;      &lt;int&gt;
##  1 police     35705
##  2 court      16191
##  3 fire       13819
##  4 crash      11103
##  5 death      11052
##  6 murder      9105
##  7 hospital    8755
##  8 accused     8017
##  9 government  7752
## 10 missing     7514
## # ... with 1,287 more rows
</code></pre>

<p>&ldquo;police&rdquo;, &ldquo;fire&rdquo;, &ldquo;crash&rdquo; are few of words associated with fear with the word &ldquo;court&rdquo; being the highest count.</p>

<h2 id="comparing-how-sentiments-differ-across-the-sentiment-libraries">Comparing how sentiments differ across the sentiment libraries</h2>

<p>There&rsquo;s three options for sentiment lexicons, let&rsquo;s see how the three sentiment lexicon differ when used for these headlines.</p>

<p>First, let&rsquo;s see how many positive and negative words each lexicon categorized.</p>

<h3 id="bing">Bing</h3>

<pre><code class="language-r">get_sentiments(&quot;bing&quot;) %&gt;% 
  count(sentiment)
</code></pre>

<pre><code>## # A tibble: 2 x 2
##   sentiment     n
##   &lt;chr&gt;     &lt;int&gt;
## 1 negative   4782
## 2 positive   2006
</code></pre>

<h3 id="nrc">NRC</h3>

<pre><code class="language-r">get_sentiments(&quot;nrc&quot;) %&gt;% 
  count(sentiment)
</code></pre>

<pre><code>## # A tibble: 10 x 2
##    sentiment        n
##    &lt;chr&gt;        &lt;int&gt;
##  1 anger         1247
##  2 anticipation   839
##  3 disgust       1058
##  4 fear          1476
##  5 joy            689
##  6 negative      3324
##  7 positive      2312
##  8 sadness       1191
##  9 surprise       534
## 10 trust         1231
</code></pre>

<ul>
<li>Bing: there are 4782 words that can be categorized as negative, and 2006 positive.</li>
<li>NRC : there are 3324 words that are categorized as negative, and 2312 positive.</li>
</ul>

<p>The proportion of negative words in Bing lexicon is much higher than proportion of negative words in NRC lexicon.</p>

<p>Let&rsquo;s count how many words in the lyrics are categorized for each sentiment:</p>

<pre><code class="language-r"># Bing lexicon
tidy_news %&gt;%
  left_join(get_sentiments(&quot;bing&quot;), by = &quot;word&quot;) %&gt;%
  group_by(sentiment) %&gt;% 
  summarize(sum = n())
</code></pre>

<pre><code>## # A tibble: 3 x 2
##   sentiment     sum
##   &lt;chr&gt;       &lt;int&gt;
## 1 negative   438340
## 2 positive   193549
## 3 &lt;NA&gt;      4652959
</code></pre>

<pre><code class="language-r"># nrc lexicon
tidy_news %&gt;% 
  left_join(get_sentiments(&quot;nrc&quot;), by = &quot;word&quot;) %&gt;% 
  group_by(sentiment) %&gt;% 
  summarize(sum = n())
</code></pre>

<pre><code>## # A tibble: 11 x 2
##    sentiment        sum
##    &lt;chr&gt;          &lt;int&gt;
##  1 anger         303608
##  2 anticipation  306593
##  3 disgust       126208
##  4 fear          434276
##  5 joy           165878
##  6 negative      549191
##  7 positive      512498
##  8 sadness       267342
##  9 surprise      148211
## 10 trust         359605
## 11 &lt;NA&gt;         3982132
</code></pre>

<ul>
<li>For Bing: 193, 549 words are categorized as negative and 193,549 words are positive.</li>
<li>For NRC: 549191 words are categorized as negative and 512,498 positive
<br /></li>
</ul>

<p>In summary, NRC lexicon managed to categorized the words much more than Bing lexicon did.</p>

<p>Let’s see how AFINN lexicon categorized the words now, as it&rsquo;s the only lexicon we haven’t touched yet in the tidytext package! The AFINN lexicon gives a score from -5 (for negative sentiment) to +5 (positive sentiment).</p>

<h3 id="afinn">AFINN</h3>

<pre><code class="language-r">headlines_afinn &lt;- tidy_news %&gt;% 
  left_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;%
  filter(!grepl('[0-9]', word))
  
# count NA category
headlines_afinn %&gt;%
    summarize(NAs= sum(is.na(score)))
</code></pre>

<pre><code>## # A tibble: 1 x 1
##       NAs
##     &lt;int&gt;
## 1 4532575
</code></pre>

<pre><code class="language-r">headlines_afinn %&gt;% 
  select(score) %&gt;% 
  mutate(sentiment = if_else(score &gt; 0, 
                             &quot;positive&quot;, &quot;negative&quot;, 
                             &quot;NA&quot;)) %&gt;% 
  group_by(sentiment) %&gt;% 
  summarize(sum = n())
</code></pre>

<pre><code>## # A tibble: 3 x 2
##   sentiment     sum
##   &lt;chr&gt;       &lt;int&gt;
## 1 NA        4532575
## 2 negative   464364
## 3 positive   202843
</code></pre>

<p>There are 4,532,575 words out of 5, 199, 782 words that was not categorized by AFINN. Let&rsquo;s visualize scoring ability of each lexicon.</p>

<pre><code class="language-r">afinn_scores &lt;- headlines_afinn %&gt;% 
  replace_na(replace = list(score = 0)) %&gt;%
  group_by(index = linenumber %/% 10000) %&gt;% 
  summarize(sentiment = sum(score)) %&gt;% 
  mutate(lexicon = &quot;AFINN&quot;)

# combine the Bing and NRC lexicons into one data frame:

bing_nrc_scores &lt;- bind_rows(
  tidy_news %&gt;% 
    inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% 
    mutate(lexicon = &quot;Bing&quot;),
  tidy_news %&gt;% 
    inner_join(get_sentiments(&quot;nrc&quot;) %&gt;% 
                 filter(sentiment %in% c(&quot;positive&quot;,
                                         &quot;negative&quot;))) %&gt;% 
    mutate(lexicon = &quot;NRC&quot;)) %&gt;% 
  # from here we count the sentiments, 
  ## then spread on positive/negative, then create the score:
  count(lexicon, index = linenumber %/% 10000, 
        sentiment) %&gt;% 
  spread(sentiment, n, fill = 0) %&gt;% 
  mutate(lexicon = as.factor(lexicon),
         sentiment = positive - negative)

# combine all lexicons into one data frame
all_lexicons &lt;- bind_rows(afinn_scores, bing_nrc_scores)
lexicon_cols &lt;- c(&quot;AFINN&quot; = &quot;#E41A1C&quot;, 
                  &quot;NRC&quot; = &quot;#377EB8&quot;, &quot;Bing&quot; = &quot;#4DAF4A&quot;)

all_lexicons
</code></pre>

<pre><code>## # A tibble: 330 x 5
##    index sentiment lexicon negative positive
##    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;
##  1  0        -5364 AFINN         NA       NA
##  2  1.00     -3761 AFINN         NA       NA
##  3  2.00     -3952 AFINN         NA       NA
##  4  3.00     -4294 AFINN         NA       NA
##  5  4.00     -4481 AFINN         NA       NA
##  6  5.00     -4077 AFINN         NA       NA
##  7  6.00     -4979 AFINN         NA       NA
##  8  7.00     -4659 AFINN         NA       NA
##  9  8.00     -5018 AFINN         NA       NA
## 10  9.00     -5087 AFINN         NA       NA
## # ... with 320 more rows
</code></pre>

<pre><code class="language-r">all_lexicons %&gt;% 
  ggplot(aes(index, sentiment, fill = lexicon)) +
  geom_col() +
  facet_wrap(~lexicon, ncol = 1, scales = &quot;free_y&quot;) +
  scale_fill_manual(values = lexicon_cols) +
  ggtitle(&quot;Comparison of Sentiments&quot;, 
          subtitle = &quot; Along Song-Order&quot;) +
  labs(x = &quot;Index of All Headlines From 2003-2017&quot;,
       y = &quot;Sentiment Score&quot;) +
  theme_bw() +
  theme(axis.text.x = element_blank())
</code></pre>

<p><img src="2017-11-26-sentiment-analysis-on-a-million-news-headlines_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>

<p>We can see that <strong>AFINN</strong> and <strong>Bing</strong> lexicon sentiment across the years have been negative, there&rsquo;s really no positive sentiment at all! But we can see in the latest index the negative score is really small, is the trend changing? we need more data to confirm that.</p>

<p>Generally, across all lexicon, the sentiment of the headlines has all been negative.</p>

<h2 id="summary">Summary</h2>

<p>We can see from the analysis that negative sentiment has been dominating media headlines in Australia since 2013 with fear being the dominating theme emotion. Most of these negative sentiment came from reporting of crime, automobile crash etc.These types of headlines are the most appealing to readers, hence why their term frequencies are high. However, the 3 lexicons we used in this analysis failed to categorized so many words in all the headlines. To be exact, there are <strong>4,652,959</strong>, <strong>3,982,132</strong>, and <strong>4,532,575</strong> words that are not categorized by <strong>Bing</strong>, <strong>NRC</strong>, and <strong>AFINN</strong> lexicon consecutively.</p>

<p>It is important to note that lexicons in the <code>tidytext</code> package are not the be all and end all for text/sentiment analysis. One can even create their own lexicons through crowd-sourcing (such as Amazon Mechanical-Turk, which is how some of the lexicons shown here were created), from utilizing word lists accrued by your own company throughout the years dealing with customer/employee feedback, etc. It would be interesting to compare this datasets with headlines from another country. For example, we can compare the most focused term used by headlines in different country using the tf-idf statistic. But bfore that, i have to build more foundational skills in text mining by going through this amazing book by Julia Silge, <a href="http://tidytextmining.com">Text Mining With R</a>.</p>

</main>

  <footer>
  <script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

  
  <hr/>
  &copy; <a href="https://akmal.netlify.com/">Akmal Abdul Rashid</a> 2017 | <a href="https://github.com/avkmal">Github</a> | <a href="https://mail.google.com/mail/?fs=1&amp;view=cm&amp;shva=1&amp;to=akmalabdulrashid@gmail.com">Email</a>
  
  </footer>
  </body>
</html>

