<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Sentiment Analysis on A Million News Headlines | A minimal Hugo website</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/about/">About</a></li>
      
      <li><a href="/categories/">Categories</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">Sentiment Analysis on A Million News Headlines</span></h1>

<h2 class="date">0001/01/01</h2>
</div>

<main>
<p>I’ve just finished <a href="http://r4ds.had.co.nz/">R for Data Science</a> by Hadley Wickham and just started <a href="http://tidytextmining.com/">Text mining With R</a> by Julia Silge. So I figured it’s about time i do some data analysis to apply the skills I learned. I decided to do sentiment analysis after reading this <a href="https://juliasilge.com/blog/life-changing-magic/">post</a> by Julia Silge.</p>
<p>After skimming through some interesting datasets on the internet, i decided to use ’A Million Headlines` dataset which can be found on Kaggle. It’s a dataset of news headlines published over a period of 14 years from 2003 to 2017 taken from Australian news source ABC(Australian Broadcasting Group).</p>
<p>First, let’s import all the packages needed:</p>
<pre class="r"><code>library(tidyverse)
library(tidytext)
library(tidyr)
library(ggplot2)
library(viridis)
library(widyr)
library(ggraph)
library(igraph)
library(data.table)
library(scales)
library(knitr)
library(wordcloud)
library(reshape2)</code></pre>
<p>First, let’s import the data</p>
<pre class="r"><code># import data
news &lt;- as.tibble(fread(&quot;C:/Users/USER/Documents/abcnews-date-text.csv&quot;))</code></pre>
<pre><code>## 
Read 63.1% of 1093281 rows
Read 91.5% of 1093281 rows
Read 1093281 rows and 2 (of 2) columns from 0.051 GB file in 00:00:04</code></pre>
<pre class="r"><code>news</code></pre>
<pre><code>## # A tibble: 1,093,281 x 2
##    publish_date                                      headline_text
##           &lt;int&gt;                                              &lt;chr&gt;
##  1     20030219 aba decides against community broadcasting licence
##  2     20030219     act fire witnesses must be aware of defamation
##  3     20030219     a g calls for infrastructure protection summit
##  4     20030219           air nz staff in aust strike for pay rise
##  5     20030219      air nz strike to affect australian travellers
##  6     20030219                  ambitious olsson wins triple jump
##  7     20030219         antic delighted with record breaking barca
##  8     20030219  aussie qualifier stosur wastes four memphis match
##  9     20030219       aust addresses un security council over iraq
## 10     20030219         australia is locked into war timetable opp
## # ... with 1,093,271 more rows</code></pre>
<div id="term-frequency" class="section level1">
<h1>Term Frequency</h1>
<p>One of the common task in text mining is to look at word frequencies. Let’s analyze word frequencies in all of the headlines</p>
<pre class="r"><code>news &lt;- news %&gt;%
      # create year column    
      mutate(year = substr(publish_date, 
                start = 1, stop = 4),
                linenumber = row_number())

news</code></pre>
<pre><code>## # A tibble: 1,093,281 x 4
##    publish_date                                      headline_text  year
##           &lt;int&gt;                                              &lt;chr&gt; &lt;chr&gt;
##  1     20030219 aba decides against community broadcasting licence  2003
##  2     20030219     act fire witnesses must be aware of defamation  2003
##  3     20030219     a g calls for infrastructure protection summit  2003
##  4     20030219           air nz staff in aust strike for pay rise  2003
##  5     20030219      air nz strike to affect australian travellers  2003
##  6     20030219                  ambitious olsson wins triple jump  2003
##  7     20030219         antic delighted with record breaking barca  2003
##  8     20030219  aussie qualifier stosur wastes four memphis match  2003
##  9     20030219       aust addresses un security council over iraq  2003
## 10     20030219         australia is locked into war timetable opp  2003
## # ... with 1,093,271 more rows, and 1 more variables: linenumber &lt;int&gt;</code></pre>
<p>we can use <code>unnest_tokens</code> to separate each line into words. The default tokenizing is for words, but other options include characters, sentences, lines, paragraphs, or separation around regex pattern.</p>
<pre class="r"><code>tidy_news &lt;- news %&gt;% 
        unnest_tokens(word, headline_text)

tidy_news</code></pre>
<pre><code>## # A tibble: 6,988,120 x 4
##    publish_date  year linenumber         word
##           &lt;int&gt; &lt;chr&gt;      &lt;int&gt;        &lt;chr&gt;
##  1     20030219  2003          1          aba
##  2     20030219  2003          1      decides
##  3     20030219  2003          1      against
##  4     20030219  2003          1    community
##  5     20030219  2003          1 broadcasting
##  6     20030219  2003          1      licence
##  7     20030219  2003          2          act
##  8     20030219  2003          2         fire
##  9     20030219  2003          2    witnesses
## 10     20030219  2003          2         must
## # ... with 6,988,110 more rows</code></pre>
<p>Now we can manipulate the data and do term frequency analysis. First, let’s remove stop words which can be obtain from dataset <code>stop_words</code> with the function <code>anti_join</code>. Stop words are words which do not contain important significance. We filter out stop words as it could affect our analysis.</p>
<pre class="r"><code># remove stopwords

data(&quot;stop_words&quot;)
tidy_news &lt;- tidy_news %&gt;%
        anti_join(stop_words)</code></pre>
<p>Let’s see the most frequent words use in the news headlines since 2003:</p>
<pre class="r"><code># most common words
tidy_news %&gt;%
    count(word, sort = TRUE) %&gt;%
    head(20) %&gt;%
    mutate(word = reorder(word, n)) %&gt;%
    ggplot(aes(word, n)) +
    geom_bar(stat = &quot;identity&quot;) +
    coord_flip() +
    ylab(&quot;Number of occurences&quot;) +
    xlab(&quot;Word&quot;)</code></pre>
<p><img src="/post/2017-11-26-sentiment-analysis-on-a-million-news-headlines_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>We can see here most of the headlines contain the words, “police”, “court”, “council”.</p>
<div id="network-of-words" class="section level2">
<h2>Network of Words</h2>
<p>Let’s count the words that occur together in the headlines from 2017. Using <code>pair_count</code> function from <code>widyr</code> package, we can count highest co-occurances pair of words.</p>
<pre class="r"><code>headlines_2017 &lt;- tidy_news %&gt;% 
    filter(year == &quot;2017&quot;) %&gt;%
    pairwise_count(word, linenumber, sort = TRUE)  
  
headlines_2017</code></pre>
<pre><code>## # A tibble: 792,786 x 3
##       item1    item2     n
##       &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt;
##  1    trump   donald   476
##  2   donald    trump   476
##  3    korea    north   238
##  4    north    korea   238
##  5 marriage      sex   156
##  6      sex marriage   156
##  7 election       wa   147
##  8       wa election   147
##  9 turnbull  malcolm   138
## 10  malcolm turnbull   138
## # ... with 792,776 more rows</code></pre>
<p>Donald trump is the highest occurences pair of words in 2017 followed by North Korea which unsurprising as the feud between them bring fear about nuclear war around the world. Also in 2017, Australia vote in favour of legalising same sex marriage which is big news across the country. Hence explains why sex marriage is just below Donald Trump and North Korea in frequency of co-occurences pair of words in 2017 headlines.</p>
<p>Let’s plot the network of words occurences:</p>
<pre class="r"><code>#pairwise count
word_pairs &lt;- tidy_news %&gt;%
  group_by(word) %&gt;%
  filter(n() &gt; 5) %&gt;%
  ungroup() %&gt;%
  pairwise_count(item=word, 
                 linenumber, sort = TRUE, 
                 upper = FALSE) %&gt;%
  filter(n &gt; 10)

#create plot
word_pairs %&gt;%
  top_n(100) %&gt;%
        graph_from_data_frame() %&gt;%
        ggraph(layout = &quot;fr&quot;) +
        geom_edge_link(aes(edge_alpha = n, 
                           edge_width = n)) +
        geom_node_point(color = &quot;darkslategray4&quot;, 
                        size = 5) +
        geom_node_text(aes(label = name), 
                       vjust = 1.8) +
        ggtitle(expression(paste(
                  &quot;Word Network in ABC Headlines From 
                  2003-2017&quot;))) +
        theme_void()</code></pre>
<p><img src="/post/2017-11-26-sentiment-analysis-on-a-million-news-headlines_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Next, we’ll look into sentiment analysis of these words so we can understand what type of sentiment have been used in most of these headlines.</p>
</div>
<div id="sentiment-analysis" class="section level2">
<h2>Sentiment Analysis</h2>
<p>Now let’s investigate sentiment analysis. When we reads a text, we use our understanding of the emotional intent of words to infer wheter a section of words is positive or negative and also categorized it into emotion like anger or joy. Let’s use bing lexicon from <code>sentiments</code> dataset to categorized our words into positive or negative sentiment.</p>
<pre class="r"><code># create dataframe of words from bing lexicon
library(tidyr)
bing &lt;- sentiments %&gt;%
        filter(lexicon == &quot;bing&quot;) %&gt;%
        select(-score)

bing</code></pre>
<pre><code>## # A tibble: 6,788 x 3
##           word sentiment lexicon
##          &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;
##  1     2-faced  negative    bing
##  2     2-faces  negative    bing
##  3          a+  positive    bing
##  4    abnormal  negative    bing
##  5     abolish  negative    bing
##  6  abominable  negative    bing
##  7  abominably  negative    bing
##  8   abominate  negative    bing
##  9 abomination  negative    bing
## 10       abort  negative    bing
## # ... with 6,778 more rows</code></pre>
<p>Using <code>inner_join</code> function, we can categorized the words into positive or negative by joining <code>bing</code> dataset.</p>
<pre class="r"><code># classified words into positive 
## or negative based on bing lexicon
news_sentiment &lt;- tidy_news %&gt;%
        inner_join(bing) %&gt;% 
        count(year,sentiment) %&gt;% 
        spread(sentiment, n, fill = 0) %&gt;% 
        mutate(sentiment = positive - negative)

news_sentiment</code></pre>
<pre><code>## # A tibble: 15 x 4
##     year negative positive sentiment
##    &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
##  1  2003    26303    12604    -13699
##  2  2004    31093    14291    -16802
##  3  2005    31705    13522    -18183
##  4  2006    28471    12123    -16348
##  5  2007    32875    13454    -19421
##  6  2008    34001    14123    -19878
##  7  2009    32679    13069    -19610
##  8  2010    31273    12589    -18684
##  9  2011    30169    11997    -18172
## 10  2012    30152    13555    -16597
## 11  2013    31885    14523    -17362
## 12  2014    28363    14290    -14073
## 13  2015    30389    14673    -15716
## 14  2016    24249    11487    -12762
## 15  2017    14733     7249     -7484</code></pre>
</div>
<div id="most-common-positive-and-negative-words" class="section level2">
<h2>Most common positive and negative words</h2>
<p>Now that we have data frame of positive and negative sentiments, we can analyze which words is most common in the positive and negative category. We can filter out <code>NA</code> sentiment or neutral sentiment.</p>
<pre class="r"><code>word_count &lt;- tidy_news %&gt;%
  left_join(get_sentiments(&quot;bing&quot;), by = &quot;word&quot;) %&gt;%
  filter(!is.na(sentiment)) %&gt;%
  count(word, sentiment, sort = TRUE) %&gt;% 
  ungroup()

top_sentiments_bing &lt;- word_count %&gt;%
          filter(word != &quot;wins&quot;) %&gt;%
          group_by(sentiment) %&gt;%
          top_n(5, n) %&gt;%
          mutate(num = ifelse(sentiment == &quot;negative&quot;,
                              -n, n)) %&gt;%
          mutate(word = reorder(word, num)) %&gt;%
          ungroup()

top_sentiments_bing</code></pre>
<pre><code>## # A tibble: 10 x 4
##       word sentiment     n    num
##      &lt;chr&gt;     &lt;chr&gt; &lt;int&gt;  &lt;int&gt;
##  1   crash  negative 11103 -11103
##  2   death  negative 11052 -11052
##  3  murder  negative  9105  -9105
##  4     win  positive  8246   8246
##  5  killed  negative  8058  -8058
##  6  attack  negative  7082  -7082
##  7   boost  positive  6979   6979
##  8    gold  positive  6161   6161
##  9     top  positive  5647   5647
## 10 support  positive  5365   5365</code></pre>
<p>Let’s see top 5 words from positive and negative sentiment.</p>
<pre class="r"><code>ggplot(top_sentiments_bing, aes(reorder(word, num), num,
                                fill = sentiment)) +
  geom_bar(stat = &#39;identity&#39;, alpha = 0.75) + 
  scale_fill_manual(guide = F, values = c(&quot;black&quot;, 
                                          &quot;darkgreen&quot;)) +
  scale_y_continuous(breaks = pretty_breaks(7)) + 
  labs(x = &#39;&#39;, y = &quot;Number of Occurrences&quot;,
       title = &#39;News Headlines Sentiments&#39;,
       subtitle = &#39;Most Common Positive and Negative Words&#39;) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1,
                                   size = 14, face = &quot;bold&quot;),
        panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_line(size = 1.1))</code></pre>
<p><img src="/post/2017-11-26-sentiment-analysis-on-a-million-news-headlines_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
<div id="word-cloud-most-common-positive-and-negative-words-in-news-headlines" class="section level2">
<h2>Word Cloud: Most Common Positive and Negative Words in News Headlines</h2>
<pre class="r"><code>library(wordcloud)   # to create wordcloud
library(reshape2)    # for acast() function

tidy_news %&gt;%
  inner_join(bing) %&gt;%
  count(word, sentiment, sort = TRUE) %&gt;%
  acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;%
  comparison.cloud(colors = c(&quot;black&quot;, &quot;darkgreen&quot;), 
                   title.size = 1.5)</code></pre>
<p><img src="/post/2017-11-26-sentiment-analysis-on-a-million-news-headlines_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
</div>
<div id="proportions-of-positive-and-negative-words" class="section level2">
<h2>Proportions of Positive and Negative Words</h2>
<p>Now let’s see the proportions of negative and positive words to entire data set. after filtering out words categorized as neutral, we calculate the frequency by first grouping them along sentiment then counting the rows for each of these groups. Finally, we can calculate the percentage by dividing the sum of all the rows in the data set.</p>
<pre class="r"><code>sentiment_bing &lt;- tidy_news %&gt;% 
        left_join(get_sentiments(&quot;bing&quot;), by = &quot;word&quot;) %&gt;%
        filter(!is.na(sentiment)) %&gt;%
        group_by(year, sentiment) %&gt;%
        summarise(n = n()) %&gt;%
        mutate(percent = n / sum(n)) %&gt;%
        ungroup()

sentiment_bing</code></pre>
<pre><code>## # A tibble: 30 x 4
##     year sentiment     n   percent
##    &lt;chr&gt;     &lt;chr&gt; &lt;int&gt;     &lt;dbl&gt;
##  1  2003  negative 26303 0.6760480
##  2  2003  positive 12604 0.3239520
##  3  2004  negative 31093 0.6851093
##  4  2004  positive 14291 0.3148907
##  5  2005  negative 31705 0.7010193
##  6  2005  positive 13522 0.2989807
##  7  2006  negative 28471 0.7013598
##  8  2006  positive 12123 0.2986402
##  9  2007  negative 32875 0.7095987
## 10  2007  positive 13454 0.2904013
## # ... with 20 more rows</code></pre>
<pre class="r"><code>sentiment_bing %&gt;% 
  ggplot(aes(x = year, y = percent, color = sentiment,
             group = sentiment)) + 
  geom_line(size = 1) + 
  geom_point(size = 3) +
  scale_y_continuous(breaks = pretty_breaks(5), 
                     labels = percent_format()) +
  labs(x = &quot;Album&quot;, y = &quot;Emotion Words Count (as %)&quot;) +
  scale_color_manual(values = c(positive = &quot;darkgreen&quot;, 
                                negative = &quot;black&quot;)) +
  ggtitle(&quot;Proportion of Positive and Negative Words&quot;, 
          subtitle = &quot;Bing lexicon&quot;) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1,
                                   size = 11, face = &quot;bold&quot;),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 11, face = &quot;bold&quot;))</code></pre>
<p><img src="/post/2017-11-26-sentiment-analysis-on-a-million-news-headlines_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>The proportion of negative sentiment words has been consistently much higher than proportion of positive sentiment words since 2003.</p>
<p>Let’s use NRC lexicon for sentimenLet’s use NRC lexicon for sentiment analysis. NRC sentiments got list of sentiments way beyond positive and negative, it categorizes words into eight different emotion terms, <code>Anger</code>, <code>Anticipation</code>, <code>Disgust</code>, <code>Fear</code>, <code>Joy</code>, <code>Sadness</code>, <code>Surprise</code>, and <code>Trust</code>.</p>
<pre class="r"><code>library(RColorBrewer)
cols &lt;- colorRampPalette(brewer.pal(n = 8, name = &quot;Set1&quot;))(8)

cols</code></pre>
<pre><code>## [1] &quot;#E41A1C&quot; &quot;#377EB8&quot; &quot;#4DAF4A&quot; &quot;#984EA3&quot; &quot;#FF7F00&quot; &quot;#FFFF33&quot; &quot;#A65628&quot;
## [8] &quot;#F781BF&quot;</code></pre>
<p>Now, let’s plot the distribution of emotion terms on boxplot:</p>
<pre class="r"><code>cols &lt;- c(&quot;anger&quot; = &quot;#E41A1C&quot;, &quot;sadness&quot; = &quot;#377EB8&quot;,
          &quot;disgust&quot; = &quot;#4DAF4A&quot;, &quot;fear&quot; = &quot;#984EA3&quot;, 
          &quot;surprise&quot; = &quot;#FF7F00&quot;, &quot;joy&quot; = &quot;#FFFF33&quot;, 
          &quot;anticipation&quot; = &quot;#A65628&quot;, &quot;trust&quot; = &quot;#F781BF&quot;)

news_nrc &lt;- tidy_news %&gt;% 
  left_join(get_sentiments(&quot;nrc&quot;), by = &quot;word&quot;) %&gt;% 
  filter(!(sentiment == &quot;negative&quot; | 
           sentiment == &quot;positive&quot;)) %&gt;% 
  mutate(sentiment = as.factor(sentiment)) %&gt;% 
  group_by(index = linenumber %/% 100, 
           sentiment) %&gt;% 
  summarize(n = n()) %&gt;% 
  mutate(percent = n / sum(n)) %&gt;%   
  select(-n) %&gt;% 
  ungroup() 

library(hrbrthemes)

news_nrc %&gt;% 
  ggplot() +
  geom_boxplot(aes(x = reorder(sentiment, percent), 
                   y = percent, fill = sentiment)) +
  scale_y_continuous(breaks = pretty_breaks(5), 
                     labels = percent_format()) +
  scale_fill_manual(values = cols) +
  ggtitle(&quot;Distribution of Emotion Terms&quot;, 
          subtitle = &quot;n = 11 (Albums)&quot;) +
  labs(x = &quot;Emotion Term&quot;, y = &quot;Percentage&quot;) +
  theme_bw() +
  theme(legend.position = &quot;none&quot;,
        axis.text.x = element_text(size = 11, 
                                   face = &quot;bold&quot;),
        axis.text.y = element_text(size = 11, 
                                   face = &quot;bold&quot;))</code></pre>
<p><img src="/post/2017-11-26-sentiment-analysis-on-a-million-news-headlines_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>Fear has highest percentage in the distribution. Next, we can see how the sentiment emotions of headlines change over time by creating bump chart that plots different sentiment groups</p>
<pre class="r"><code>news_nrc2 &lt;- tidy_news %&gt;% 
  left_join(get_sentiments(&quot;nrc&quot;), by = &quot;word&quot;) %&gt;% 
  filter(!(sentiment == &quot;negative&quot; | 
             sentiment == &quot;positive&quot;)) %&gt;% 
  mutate(sentiment = as.factor(sentiment)) %&gt;% 
  group_by(year, sentiment) %&gt;% 
  summarize(n = n()) %&gt;% 
  mutate(percent = n / sum(n)) %&gt;%   
  select(-n) %&gt;% 
  ungroup() 


news_nrc2 %&gt;% 
  group_by(year) %&gt;%
  ggplot(aes(year, percent, color = sentiment, 
             group = sentiment)) +
  geom_line(size = 1) +
  geom_point(size = 3.5) +
  scale_y_continuous(breaks = pretty_breaks(5), 
                     labels = percent_format()) +
  xlab(&quot;Year&quot;) + ylab(&quot;Proportion of Emotion Words&quot;) +
  ggtitle(&quot;News Headlines Sentiments Across Years&quot;, 
          subtitle = &quot;From 2000-2016&quot;) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, 
                                   hjust = 1, size = 11, 
                                   face = &quot;bold&quot;),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 11, 
                                   face = &quot;bold&quot;)) +
  scale_color_brewer(palette = &quot;Set1&quot;)</code></pre>
<p><img src="/post/2017-11-26-sentiment-analysis-on-a-million-news-headlines_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>We can see that the sentiment changes over time is quite consistent, with fear sentiment already at high level in 2003.</p>
<p>Let’s see what are the most words used that are associated with fear:</p>
<pre class="r"><code>nrc_fear &lt;- get_sentiments(&quot;nrc&quot;) %&gt;% 
  filter(sentiment == &quot;fear&quot;)

tidy_news %&gt;% 
  inner_join(nrc_fear) %&gt;% 
  count(word, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 1,297 x 2
##          word     n
##         &lt;chr&gt; &lt;int&gt;
##  1     police 35705
##  2      court 16191
##  3       fire 13819
##  4      crash 11103
##  5      death 11052
##  6     murder  9105
##  7   hospital  8755
##  8    accused  8017
##  9 government  7752
## 10    missing  7514
## # ... with 1,287 more rows</code></pre>
<p>“police”, “fire”, “crash” are few of words associated with fear with the word “court” being the highest count.</p>
</div>
<div id="comparing-how-sentiments-differ-across-the-sentiment-libraries" class="section level2">
<h2>Comparing how sentiments differ across the sentiment libraries</h2>
<p>There’s three options for sentiment lexicons, let’s see how the three sentiment lexicon differ when used for these headlines.</p>
<p>First, let’s see how many positive and negative words each lexicon categorized.</p>
<div id="bing" class="section level3">
<h3>Bing</h3>
<pre class="r"><code>get_sentiments(&quot;bing&quot;) %&gt;% 
  count(sentiment)</code></pre>
<pre><code>## # A tibble: 2 x 2
##   sentiment     n
##       &lt;chr&gt; &lt;int&gt;
## 1  negative  4782
## 2  positive  2006</code></pre>
</div>
<div id="nrc" class="section level3">
<h3>NRC</h3>
<pre class="r"><code>get_sentiments(&quot;nrc&quot;) %&gt;% 
  count(sentiment)</code></pre>
<pre><code>## # A tibble: 10 x 2
##       sentiment     n
##           &lt;chr&gt; &lt;int&gt;
##  1        anger  1247
##  2 anticipation   839
##  3      disgust  1058
##  4         fear  1476
##  5          joy   689
##  6     negative  3324
##  7     positive  2312
##  8      sadness  1191
##  9     surprise   534
## 10        trust  1231</code></pre>
<ul>
<li>Bing: there are 4782 words that can be categorized as negative, and 2006 positive.</li>
<li>NRC : there are 3324 words that are categorized as negative, and 2312 positive.</li>
</ul>
<p>The proportion of negative words in Bing lexicon is much higher than proportion of negative words in NRC lexicon.</p>
<p>Let’s count how many words in the lyrics are categorized for each sentiment:</p>
<pre class="r"><code># Bing lexicon
tidy_news %&gt;%
  left_join(get_sentiments(&quot;bing&quot;), by = &quot;word&quot;) %&gt;%
  group_by(sentiment) %&gt;% 
  summarize(sum = n())</code></pre>
<pre><code>## # A tibble: 3 x 2
##   sentiment     sum
##       &lt;chr&gt;   &lt;int&gt;
## 1  negative  438340
## 2  positive  193549
## 3      &lt;NA&gt; 4652959</code></pre>
<pre class="r"><code># nrc lexicon
tidy_news %&gt;% 
  left_join(get_sentiments(&quot;nrc&quot;), by = &quot;word&quot;) %&gt;% 
  group_by(sentiment) %&gt;% 
  summarize(sum = n())</code></pre>
<pre><code>## # A tibble: 11 x 2
##       sentiment     sum
##           &lt;chr&gt;   &lt;int&gt;
##  1        anger  303608
##  2 anticipation  306593
##  3      disgust  126208
##  4         fear  434276
##  5          joy  165878
##  6     negative  549191
##  7     positive  512498
##  8      sadness  267342
##  9     surprise  148211
## 10        trust  359605
## 11         &lt;NA&gt; 3982132</code></pre>
<ul>
<li>For Bing: 193, 549 words are categorized as negative and 193,549 words are positive.</li>
<li>For NRC: 549191 words are categorized as negative and 512,498 positive</li>
</ul>
<p>In summary, NRC lexicon managed to categorized the words much more than Bing lexicon did.</p>
<p>Let’s see how AFINN lexicon categorized the words now, as it’s the only lexicon we haven’t touched yet in the tidytext package! The AFINN lexicon gives a score from -5 (for negative sentiment) to +5 (positive sentiment).</p>
</div>
<div id="afinn" class="section level3">
<h3>AFINN</h3>
<pre class="r"><code>headlines_afinn &lt;- tidy_news %&gt;% 
  left_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;%
  filter(!grepl(&#39;[0-9]&#39;, word))
  
# count NA category
headlines_afinn %&gt;%
    summarize(NAs= sum(is.na(score)))</code></pre>
<pre><code>## # A tibble: 1 x 1
##       NAs
##     &lt;int&gt;
## 1 4532575</code></pre>
<pre class="r"><code>headlines_afinn %&gt;% 
  select(score) %&gt;% 
  mutate(sentiment = if_else(score &gt; 0, 
                             &quot;positive&quot;, &quot;negative&quot;, 
                             &quot;NA&quot;)) %&gt;% 
  group_by(sentiment) %&gt;% 
  summarize(sum = n())</code></pre>
<pre><code>## # A tibble: 3 x 2
##   sentiment     sum
##       &lt;chr&gt;   &lt;int&gt;
## 1        NA 4532575
## 2  negative  464364
## 3  positive  202843</code></pre>
<p>There are 4,532,575 words out of 5, 199, 782 words that was not categorized by AFINN. Let’s visualize scoring ability of each lexicon.</p>
<pre class="r"><code>afinn_scores &lt;- headlines_afinn %&gt;% 
  replace_na(replace = list(score = 0)) %&gt;%
  group_by(index = linenumber %/% 10000) %&gt;% 
  summarize(sentiment = sum(score)) %&gt;% 
  mutate(lexicon = &quot;AFINN&quot;)

# combine the Bing and NRC lexicons into one data frame:

bing_nrc_scores &lt;- bind_rows(
  tidy_news %&gt;% 
    inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% 
    mutate(lexicon = &quot;Bing&quot;),
  tidy_news %&gt;% 
    inner_join(get_sentiments(&quot;nrc&quot;) %&gt;% 
                 filter(sentiment %in% c(&quot;positive&quot;,
                                         &quot;negative&quot;))) %&gt;% 
    mutate(lexicon = &quot;NRC&quot;)) %&gt;% 
  # from here we count the sentiments, 
  ## then spread on positive/negative, then create the score:
  count(lexicon, index = linenumber %/% 10000, 
        sentiment) %&gt;% 
  spread(sentiment, n, fill = 0) %&gt;% 
  mutate(lexicon = as.factor(lexicon),
         sentiment = positive - negative)

# combine all lexicons into one data frame
all_lexicons &lt;- bind_rows(afinn_scores, bing_nrc_scores)
lexicon_cols &lt;- c(&quot;AFINN&quot; = &quot;#E41A1C&quot;, 
                  &quot;NRC&quot; = &quot;#377EB8&quot;, &quot;Bing&quot; = &quot;#4DAF4A&quot;)

all_lexicons</code></pre>
<pre><code>## # A tibble: 330 x 5
##    index sentiment lexicon negative positive
##    &lt;dbl&gt;     &lt;dbl&gt;   &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;
##  1     0     -5364   AFINN       NA       NA
##  2     1     -3761   AFINN       NA       NA
##  3     2     -3952   AFINN       NA       NA
##  4     3     -4294   AFINN       NA       NA
##  5     4     -4481   AFINN       NA       NA
##  6     5     -4077   AFINN       NA       NA
##  7     6     -4979   AFINN       NA       NA
##  8     7     -4659   AFINN       NA       NA
##  9     8     -5018   AFINN       NA       NA
## 10     9     -5087   AFINN       NA       NA
## # ... with 320 more rows</code></pre>
<pre class="r"><code>all_lexicons %&gt;% 
  ggplot(aes(index, sentiment, fill = lexicon)) +
  geom_col() +
  facet_wrap(~lexicon, ncol = 1, scales = &quot;free_y&quot;) +
  scale_fill_manual(values = lexicon_cols) +
  ggtitle(&quot;Comparison of Sentiments&quot;, 
          subtitle = &quot; Along Song-Order&quot;) +
  labs(x = &quot;Index of All Headlines From 2003-2017&quot;,
       y = &quot;Sentiment Score&quot;) +
  theme_bw() +
  theme(axis.text.x = element_blank())</code></pre>
<p><img src="/post/2017-11-26-sentiment-analysis-on-a-million-news-headlines_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>We can see that <strong>AFINN</strong> and <strong>Bing</strong> lexicon sentiment across the years have been negative, there’s really no positive sentiment at all! But we can see in the latest index the negative score is really small, is the trend changing? we need more data to confirm that.</p>
<p>Generally, across all lexicon, the sentiment of the headlines has all been negative.</p>
</div>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<p>We can see from the analysis that negative sentiment has been dominating media headlines in Australia since 2013 with fear being the dominating theme emotion. Most of these negative sentiment came from reporting of crime, automobile crash etc.These types of headlines are the most appealing to readers, hence why their term frequencies are high. However, the 3 lexicons we used in this analysis failed to categorized so many words in all the headlines. To be exact, there are <strong>4,652,959</strong>, <strong>3,982,132</strong>, and <strong>4,532,575</strong> words that are not categorized by <strong>Bing</strong>, <strong>NRC</strong>, and <strong>AFINN</strong> lexicon consecutively.</p>
<p>It is important to note that lexicons in the <code>tidytext</code> package are not the be all and end all for text/sentiment analysis. One can even create their own lexicons through crowd-sourcing (such as Amazon Mechanical-Turk, which is how some of the lexicons shown here were created), from utilizing word lists accrued by your own company throughout the years dealing with customer/employee feedback, etc. It would be interesting to compare this datasets with headlines from another country. For example, we can compare the most focused term used by headlines in different country using the tf-idf statistic.</p>
</div>
</div>

</main>

  <footer>
  <script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

  
  <hr/>
  &copy; <a href="https://akmal.netlify.com/">Akmal Abdul Rashid</a> 2017 | <a href="https://github.com/avkmal">Github</a> | <a href="https://twitter.com/_avkmal">Twitter</a>
  
  </footer>
  </body>
</html>

