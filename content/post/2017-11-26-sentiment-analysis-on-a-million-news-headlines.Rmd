---
title: Sentiment Analysis of A Million News Headlines
author: ''
date: '2017-11-26'
slug: sentiment-analysis-of-a-million-news-headlines
categories: [Project]
tags: [Sentiment Analysis, Text Mining]
---

I've just finished [R for Data Science](http://r4ds.had.co.nz/) by Hadley Wickham and just started [Text mining With R](http://tidytextmining.com/) by Julia Silge. So I figured it's about time i do some data analysis to apply the skills I learned. I decided to do sentiment analysis after reading this [post](https://juliasilge.com/blog/life-changing-magic/) by Julia Silge.

After skimming through some interesting datasets on the internet, i decided to use 'A Million Headlines` dataset which can be found on Kaggle. It's a dataset of news headlines published over a period of 14 years from 2003 to 2017 taken from Australian news source ABC(Australian Broadcasting Group).

First, let's import all the packages needed:

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(here)
library(tidytext)
library(viridis)
library(widyr)
library(ggraph)
library(igraph)
library(scales)
library(knitr)
library(wordcloud)
library(reshape2)
```

Now let's import the data

```{r message=FALSE}
# import data
news <- as.tibble(read_csv("C:/Users/USER/Documents/abcnews-date-text.csv"))
news
```

# Term Frequency

One of the common task in text mining is to look at word frequencies. Let's analyze word frequencies in all of the headlines

```{r message=FALSE, warning=FALSE}
news <- news %>%
      # create year column    
      mutate(year = substr(publish_date, 
                start = 1, stop = 4),
                linenumber = row_number())

news
```

we can use `unnest_tokens` to separate each line into words. The default tokenizing is for words, but other options include characters, sentences, lines, paragraphs, or separation around regex pattern.


```{r}
tidy_news <- news %>% 
        unnest_tokens(word, headline_text)

tidy_news
```

Now we can manipulate the data and do term frequency analysis. First, let's remove stop words which can be obtain from dataset `stop_words` with the function `anti_join`. Stop words are words which do not contain important significance. We filter out stop words as it could affect our analysis.

```{r message=FALSE}
# remove stopwords

data("stop_words")
tidy_news <- tidy_news %>%
        anti_join(stop_words)
```

Let's see the most frequent words use in the news headlines since 2003:

```{r message=FALSE}
# most common words
tidy_news %>%
    count(word, sort = TRUE) %>%
    head(20) %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(word, n)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    ylab("Number of occurences") +
    xlab("Word")
```

We can see here most of the headlines contain the words, "police", "court", "council".


## Network of Words

Let's count the words that occur together in the headlines from 2017. Using `pair_count` function from `widyr` package, we can count highest co-occurances pair of words.

```{r message=FALSE}
headlines_2017 <- tidy_news %>% 
    filter(year == "2017") %>%
    pairwise_count(word, linenumber, sort = TRUE)  
  
headlines_2017
```

Donald trump is the highest occurences pair of words in 2017 followed by North Korea which unsurprising as the feud between them bring fear about nuclear war around the world. Also in 2017, Australia vote in favour of legalising same sex marriage which is big news across the country. Hence explains why sex marriage is just below Donald Trump and North Korea in frequency of co-occurences pair of words in 2017 headlines.


Let's plot the network of words occurences:

```{r message=FALSE, warnings=FALSE}
#pairwise count
word_pairs <- tidy_news %>%
  group_by(word) %>%
  filter(n() > 5) %>%
  ungroup() %>%
  pairwise_count(item=word, 
                 linenumber, sort = TRUE, 
                 upper = FALSE) %>%
  filter(n > 10)

#create plot
word_pairs %>%
  top_n(100) %>%
        graph_from_data_frame() %>%
        ggraph(layout = "fr") +
        geom_edge_link(aes(edge_alpha = n, 
                           edge_width = n)) +
        geom_node_point(color = "darkslategray4", 
                        size = 5) +
        geom_node_text(aes(label = name), 
                       vjust = 1.8) +
        ggtitle(expression(paste(
                  "Word Network in ABC Headlines From 
                  2003-2017"))) +
        theme_void()
```

Next, we'll look into sentiment analysis of these words so we can understand what type of sentiment have been used in most of these headlines.

## Sentiment Analysis

Now let's investigate sentiment analysis. When we reads a text, we use our understanding of the emotional intent of words to infer wheter a section of words is positive or negative and also categorized it into emotion like anger or joy. Let's use bing lexicon from `sentiments` dataset to categorized our words into positive or negative sentiment. 

```{r}
# create dataframe of words from bing lexicon
library(tidyr)
bing <- sentiments %>%
        filter(lexicon == "bing") %>%
        select(-score)

bing
```

Using `inner_join` function, we can categorized the words into positive or negative by joining `bing` dataset.

```{r message=FALSE}
# classified words into positive 
## or negative based on bing lexicon
news_sentiment <- tidy_news %>%
        inner_join(bing) %>% 
        count(year,sentiment) %>% 
        spread(sentiment, n, fill = 0) %>% 
        mutate(sentiment = positive - negative)

news_sentiment
```

## Most common positive and negative words

Now that we have data frame of positive and negative sentiments, we can analyze which words is most common in the positive and negative category. We can filter out `NA` sentiment or neutral sentiment. 

```{r warning=FALSE}
word_count <- tidy_news %>%
  left_join(get_sentiments("bing"), by = "word") %>%
  filter(!is.na(sentiment)) %>%
  count(word, sentiment, sort = TRUE) %>% 
  ungroup()

top_sentiments_bing <- word_count %>%
          filter(word != "wins") %>%
          group_by(sentiment) %>%
          top_n(5, n) %>%
          mutate(num = ifelse(sentiment == "negative",
                              -n, n)) %>%
          mutate(word = reorder(word, num)) %>%
          ungroup()

top_sentiments_bing
```

Let's see top 5 words from positive and negative sentiment.

```{r}
ggplot(top_sentiments_bing, aes(reorder(word, num), num,
                                fill = sentiment)) +
  geom_bar(stat = 'identity', alpha = 0.75) + 
  scale_fill_manual(guide = F, values = c("black", 
                                          "darkgreen")) +
  scale_y_continuous(breaks = pretty_breaks(7)) + 
  labs(x = '', y = "Number of Occurrences",
       title = 'News Headlines Sentiments',
       subtitle = 'Most Common Positive and Negative Words') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1,
                                   size = 14, face = "bold"),
        panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_line(size = 1.1))
```

## Word Cloud: Most Common Positive and Negative Words in News Headlines

```{r message=FALSE, warning=FALSE}
library(wordcloud)   # to create wordcloud
library(reshape2)    # for acast() function

tidy_news %>%
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("black", "darkgreen"), 
                   title.size = 1.5)
```

## Proportions of Positive and Negative Words

Now let's see the proportions of negative and positive words to entire data set. after filtering out words categorized as neutral, we calculate the frequency by first grouping them along sentiment then counting the rows for each of these groups. Finally, we can calculate the percentage by dividing the sum of all the rows in the data set.

```{r}
sentiment_bing <- tidy_news %>% 
        left_join(get_sentiments("bing"), by = "word") %>%
        filter(!is.na(sentiment)) %>%
        group_by(year, sentiment) %>%
        summarise(n = n()) %>%
        mutate(percent = n / sum(n)) %>%
        ungroup()

sentiment_bing
```

```{r}
sentiment_bing %>% 
  ggplot(aes(x = year, y = percent, color = sentiment,
             group = sentiment)) + 
  geom_line(size = 1) + 
  geom_point(size = 3) +
  scale_y_continuous(breaks = pretty_breaks(5), 
                     labels = percent_format()) +
  labs(x = "Album", y = "Emotion Words Count (as %)") +
  scale_color_manual(values = c(positive = "darkgreen", 
                                negative = "black")) +
  ggtitle("Proportion of Positive and Negative Words", 
          subtitle = "Bing lexicon") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1,
                                   size = 11, face = "bold"),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 11, face = "bold"))
```

The proportion of negative sentiment words has been consistently much higher than proportion of positive sentiment words since 2003.



Let's use NRC lexicon for sentimenLet's use NRC lexicon for sentiment analysis. NRC sentiments got list of sentiments way beyond positive and negative, it categorizes words into eight different emotion terms, `Anger`, `Anticipation`, `Disgust`, `Fear`, `Joy`, `Sadness`, `Surprise`, and `Trust`.

```{r}
library(RColorBrewer)
cols <- colorRampPalette(brewer.pal(n = 8, name = "Set1"))(8)

cols
```

Now, let's plot the distribution of emotion terms on boxplot:

```{r message=FALSE, warning=FALSE}
cols <- c("anger" = "#E41A1C", "sadness" = "#377EB8",
          "disgust" = "#4DAF4A", "fear" = "#984EA3", 
          "surprise" = "#FF7F00", "joy" = "#FFFF33", 
          "anticipation" = "#A65628", "trust" = "#F781BF")

news_nrc <- tidy_news %>% 
  left_join(get_sentiments("nrc"), by = "word") %>% 
  filter(!(sentiment == "negative" | 
           sentiment == "positive")) %>% 
  mutate(sentiment = as.factor(sentiment)) %>% 
  group_by(index = linenumber %/% 100, 
           sentiment) %>% 
  summarize(n = n()) %>% 
  mutate(percent = n / sum(n)) %>%   
  select(-n) %>% 
  ungroup() 

library(hrbrthemes)

news_nrc %>% 
  ggplot() +
  geom_boxplot(aes(x = reorder(sentiment, percent), 
                   y = percent, fill = sentiment)) +
  scale_y_continuous(breaks = pretty_breaks(5), 
                     labels = percent_format()) +
  scale_fill_manual(values = cols) +
  ggtitle("Distribution of Emotion Terms", 
          subtitle = "n = 11 (Albums)") +
  labs(x = "Emotion Term", y = "Percentage") +
  theme_bw() +
  theme(legend.position = "none",
        axis.text.x = element_text(size = 11, 
                                   face = "bold"),
        axis.text.y = element_text(size = 11, 
                                   face = "bold"))
```

Fear has highest percentage in the distribution. Next, we can see how the sentiment emotions of headlines change over time by creating bump chart that plots different sentiment groups 

```{r}
news_nrc2 <- tidy_news %>% 
  left_join(get_sentiments("nrc"), by = "word") %>% 
  filter(!(sentiment == "negative" | 
             sentiment == "positive")) %>% 
  mutate(sentiment = as.factor(sentiment)) %>% 
  group_by(year, sentiment) %>% 
  summarize(n = n()) %>% 
  mutate(percent = n / sum(n)) %>%   
  select(-n) %>% 
  ungroup() 


news_nrc2 %>% 
  group_by(year) %>%
  ggplot(aes(year, percent, color = sentiment, 
             group = sentiment)) +
  geom_line(size = 1) +
  geom_point(size = 3.5) +
  scale_y_continuous(breaks = pretty_breaks(5), 
                     labels = percent_format()) +
  xlab("Year") + ylab("Proportion of Emotion Words") +
  ggtitle("News Headlines Sentiments Across Years", 
          subtitle = "From 2000-2016") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, 
                                   hjust = 1, size = 11, 
                                   face = "bold"),
        axis.title.x = element_blank(),
        axis.text.y = element_text(size = 11, 
                                   face = "bold")) +
  scale_color_brewer(palette = "Set1")
```

We can see that the sentiment changes over time is quite consistent, with fear sentiment already at high level in 2003.

Let's see what are the most words used that are associated with fear:

```{r message=FALSE, warning=FALSE}
nrc_fear <- get_sentiments("nrc") %>% 
  filter(sentiment == "fear")

tidy_news %>% 
  inner_join(nrc_fear) %>% 
  count(word, sort = TRUE)
```

"police", "fire", "crash" are few of words associated with fear with the word "court" being the highest count.

## Comparing how sentiments differ across the sentiment libraries

There's three options for sentiment lexicons, let's see how the three sentiment lexicon differ when used for these headlines.

First, let's see how many positive and negative words each lexicon categorized.

### Bing 

```{r}
get_sentiments("bing") %>% 
  count(sentiment)
```

### NRC

```{r}
get_sentiments("nrc") %>% 
  count(sentiment)
```

  * Bing: there are 4782 words that can be categorized as negative, and 2006 positive.
  * NRC : there are 3324 words that are categorized as negative, and 2312 positive.

The proportion of negative words in Bing lexicon is much higher than proportion of negative words in NRC lexicon.

Let's count how many words in the lyrics are categorized for each sentiment:

```{r}
# Bing lexicon
tidy_news %>%
  left_join(get_sentiments("bing"), by = "word") %>%
  group_by(sentiment) %>% 
  summarize(sum = n())
```

```{r}
# nrc lexicon
tidy_news %>% 
  left_join(get_sentiments("nrc"), by = "word") %>% 
  group_by(sentiment) %>% 
  summarize(sum = n())
```

  * For Bing: 193, 549 words are categorized as negative and 193,549 words are positive.
  * For NRC: 549191 words are categorized as negative and 512,498 positive 
  
In summary, NRC lexicon managed to categorized the words much more than Bing lexicon did. 

Let’s see how AFINN lexicon categorized the words now, as it's the only lexicon we haven’t touched yet in the tidytext package! The AFINN lexicon gives a score from -5 (for negative sentiment) to +5 (positive sentiment).

### AFINN

```{r}
headlines_afinn <- tidy_news %>% 
  left_join(get_sentiments("afinn"), by = "word") %>%
  filter(!grepl('[0-9]', word))
  
# count NA category
headlines_afinn %>%
    summarize(NAs= sum(is.na(score)))
```

```{r}
headlines_afinn %>% 
  select(score) %>% 
  mutate(sentiment = if_else(score > 0, 
                             "positive", "negative", 
                             "NA")) %>% 
  group_by(sentiment) %>% 
  summarize(sum = n())
```

There are 4,532,575 words out of 5, 199, 782 words that was not categorized by AFINN. Let's visualize scoring ability of each lexicon.

```{r message=FALSE, warning=FALSE}
afinn_scores <- headlines_afinn %>% 
  replace_na(replace = list(score = 0)) %>%
  group_by(index = linenumber %/% 10000) %>% 
  summarize(sentiment = sum(score)) %>% 
  mutate(lexicon = "AFINN")

# combine the Bing and NRC lexicons into one data frame:

bing_nrc_scores <- bind_rows(
  tidy_news %>% 
    inner_join(get_sentiments("bing")) %>% 
    mutate(lexicon = "Bing"),
  tidy_news %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive",
                                         "negative"))) %>% 
    mutate(lexicon = "NRC")) %>% 
  # from here we count the sentiments, 
  ## then spread on positive/negative, then create the score:
  count(lexicon, index = linenumber %/% 10000, 
        sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(lexicon = as.factor(lexicon),
         sentiment = positive - negative)

# combine all lexicons into one data frame
all_lexicons <- bind_rows(afinn_scores, bing_nrc_scores)
lexicon_cols <- c("AFINN" = "#E41A1C", 
                  "NRC" = "#377EB8", "Bing" = "#4DAF4A")

all_lexicons
```

```{r}
all_lexicons %>% 
  ggplot(aes(index, sentiment, fill = lexicon)) +
  geom_col() +
  facet_wrap(~lexicon, ncol = 1, scales = "free_y") +
  scale_fill_manual(values = lexicon_cols) +
  ggtitle("Comparison of Sentiments", 
          subtitle = " Along Song-Order") +
  labs(x = "Index of All Headlines From 2003-2017",
       y = "Sentiment Score") +
  theme_bw() +
  theme(axis.text.x = element_blank())
```

We can see that **AFINN** and **Bing** lexicon sentiment across the years have been negative, there's really no positive sentiment at all! But we can see in the latest index the negative score is really small, is the trend changing? we need more data to confirm that.

Generally, across all lexicon, the sentiment of the headlines has all been negative.

## Summary

We can see from the analysis that negative sentiment has been dominating media headlines in Australia since 2013 with fear being the dominating theme emotion. Most of these negative sentiment came from reporting of crime, automobile crash etc.These types of headlines are the most appealing to readers, hence why their term frequencies are high. However, the 3 lexicons we used in this analysis failed to categorized so many words in all the headlines. To be exact, there are **4,652,959**, **3,982,132**, and **4,532,575** words that are not categorized by **Bing**, **NRC**, and **AFINN** lexicon consecutively. 

It is important to note that lexicons in the `tidytext` package are not the be all and end all for text/sentiment analysis. One can even create their own lexicons through crowd-sourcing (such as Amazon Mechanical-Turk, which is how some of the lexicons shown here were created), from utilizing word lists accrued by your own company throughout the years dealing with customer/employee feedback, etc. It would be interesting to compare this datasets with headlines from another country. For example, we can compare the most focused term used by headlines in different country using the tf-idf statistic. But bfore that, i have to build more foundational skills in text mining by going through this amazing book by Julia Silge, [Text Mining With R](http://tidytextmining.com).
